#DATA
data_root: /data/local/yg397/dataset/universal/npy
#MODEL
arch: medformer
in_chan: 1
base_chan: 32
conv_block: 'BasicBlock'

down_scale: [[2,2,2], [2,2,2], [2,2,2], [2,2,2]]
kernel_size: [[3,3,3], [3,3,3], [3,3,3], [3,3,3], [3,3,3]]
chan_num: [64, 128, 256, 320, 256, 128, 64, 32]
norm: in
act: relu
map_size: [4, 4, 4]
conv_num: [2,0,0,0, 0,0,2,2]
trans_num: [0,2,4,8, 4,2,0,0]
num_heads: [1,4,8,10, 8,4,1,1]
expansion: 4
fusion_depth: 2
fusion_dim: 320
fusion_heads: 10
attn_drop: 0.
proj_drop: 0.
proj_type: 'depthwise'
tn: 72  # the number of task priors + one null task token
mn: 6  # the number of modality priors

#TRAIN
epochs: 200
training_size: [128, 128, 128] # training crop size
start_epoch: 0
num_workers: 2
aug_device: 'cpu'
aux_loss: True
aux_weight: [0.5, 0.5]

dataset_name_list: ['amos_ct', 'amos_mr', 'bcv', 'structseg_oar', 'structseg_head_oar', 'lits', 'kits', 'chaos', 'mnm', 'brain_structure', 'autopet']
dataset_classes_list: [15, 13, 13, 6, 22, 2, 2, 4, 3, 3, 1] #the number of classes of each dataset, exclude background. amos_ct and amos_mr share the same tasks, total 71 tasks
split_seed: 0
k_fold: 1

optimizer: lamb
base_lr: 0.002
betas: [0.9, 0.999]
warmup_epoch: 0
weight_decay: 0.05  # weight decay of the optimizer
rlt: 1 # relation between CE and Dice loss
loss_mod_weight: 0.01

scale: [0.3, 0.3, 0.3]  # scale for data augmentation
rotate: [30, 30, 30] # rotation angle for data augmentation 
translate: [0, 0, 0]
gaussian_noise_std: 0.02

print_freq: 5
iter_per_epoch: 400




#VALIDATION
ema: True
ema_alpha: 0.99
val_freq: 25



#INFERENCE
sliding_window: True
window_size: [128, 128, 128]


# DDP
world_size: 1
proc_idx: 0
rank: 0
port: 10010
dist_url: 'tcp://localhost:10000' # the port number here should be the same as the previous one
dist_backend: "nccl"
multiprocessing_distributed: true
reproduce_seed: null
